{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Y-Mj7dv5JJM2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Actions de configuration\n",
        "- Créer un répertoire image-heberg.fr dans son drive\n",
        "- Créer un fichier key.web contenant la clef `_KEY_FOR_IA_TRAINING_` configurée dans le fichier config.php"
      ],
      "metadata": {
        "id": "h7LvLfJ9gyha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sources :\n",
        "  - [Coder un réseau de neurones convolutifs de classification d'image avec Python et Tensorflow](https://www.youtube.com/watch?v=6FHtTyZxS5s)\n",
        "  - [Notebook Jupyter associé](https://github.com/anisayari/Youtube-apprendre-le-deeplearning-avec-tensorflow/blob/master/%234%20-%20CNN/A_deep_introduction_to_CNN%20(1).ipynb)\n",
        "  - [Connexion à Google Drive](https://colab.research.google.com/drive/1znbKFQs98XolesqOZzseFjOLVPkGoXiq?usp=sharing#scrollTo=RIz91NnKbq-j)\n",
        "  - [TensorflowJS](https://www.tensorflow.org/js/tutorials/conversion/import_keras?hl=fr)"
      ],
      "metadata": {
        "id": "6NijVuHVUKyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "luiAwX47g_5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Charger toutes les libs et modules requis pour le projet"
      ],
      "metadata": {
        "id": "riFE3y2wRorX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Infos sur l'environnement\n",
        "!apt install pciutils --quiet\n",
        "!lspci | grep -i nvidia\n",
        "!uname -m && cat /etc/*release\n",
        "\n",
        "# Installation de cuda\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n",
        "!sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
        "!apt install cuda-toolkit cuda-drivers cudnn\n",
        "\n",
        "# Vérifier l'installation de cuda\n",
        "!wget https://github.com/NVIDIA/cuda-samples/archive/refs/heads/master.zip\n",
        "!unzip master.zip\n",
        "!apt install cmake\n",
        "!python3 -m pip install cmake\n",
        "!cd cuda-samples-master && make\n",
        "!./cuda-samples-master/bin/x86_64/linux/release/deviceQuery\n",
        "\n",
        "# Forcer le passage à Python >=3.11 pour pouvoir utiliser hashlib.file_digest() (actuellement Python3.10)\n",
        "!python3 --version\n",
        "import sys, os\n",
        "print(sys.version)\n",
        "\n",
        "# Adapté de https://stackoverflow.com/a/74538231\n",
        "#install python 3.11 and dev utils\n",
        "#you may not need all the dev libraries, but I haven't tested which aren't necessary.\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.11 python3.11-dev python3.11-distutils libpython3.11-dev python3.11-venv binfmt-support\n",
        "\n",
        "#change alternatives\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1\n",
        "\n",
        "# install pip\n",
        "!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "!python3 get-pip.py --force-reinstall\n",
        "\n",
        "#install colab's dependencies\n",
        "!python3 -m pip install ipython traitlets jupyter psutil matplotlib setuptools ipython_genutils ipykernel jupyter_console notebook prompt_toolkit httplib2 astor\n",
        "\n",
        "# Installation des dépendances de la suite de mes scripts\n",
        "!python3 -m pip install opencv-python\n",
        "\n",
        "# link to the old google package\n",
        "!ln -s /usr/local/lib/python3.10/dist-packages/google /usr/local/lib/python3.11/dist-packages/google\n",
        "\n",
        "# There has got to be a better way to do this...but there's a bad import in some of the colab files\n",
        "# IPython no longer exposes traitlets like this, it's a separate package now\n",
        "!sed -i \"s/from IPython.utils import traitlets as _traitlets/import traitlets as _traitlets/\" /usr/local/lib/python3.11/dist-packages/google/colab/*.py\n",
        "!sed -i \"s/from IPython.utils import traitlets/import traitlets/\" /usr/local/lib/python3.11/dist-packages/google/colab/*.py\n",
        "\n",
        "# Vérifier les versions\n",
        "!python3 --version\n",
        "import sys, os\n",
        "print(sys.version)\n",
        "\n",
        "#restart environment so you don't have to do it manually\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "C7FfkR3QvxAJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier les versions\n",
        "!python3 --version\n",
        "import sys, os\n",
        "print(sys.version)\n",
        "\n",
        "# Vérifier qu'on voit bien le GPU\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\n",
        "!nvidia-smi\n",
        "\n",
        "# Import de toutes les libs requises dans le projet\n",
        "import datetime\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "!python3 -m pip install numpy tensorflow[and-cuda] tensorflowjs --use-deprecated=legacy-resolver\n",
        "# Tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "# TensorFlowJS (pour l'export du modèle)\n",
        "import tensorflowjs as tfjs\n",
        "# Affichage des images\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "# Réseaux\n",
        "from http.client import IncompleteRead\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "from urllib.request import urlretrieve, URLError\n",
        "# Enregistrer sur le Google Drive associé\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "# Chiffrement des fichiers sur le Google Drive (confidentialité + CGU)\n",
        "!python3 -m pip install cryptography --quiet\n",
        "from cryptography.fernet import Fernet\n",
        "# Vérification des images\n",
        "import hashlib\n",
        "\n",
        "print(sys.version)\n",
        "print('TensorFlow :',tf.__version__)\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "3wj8ELVPMEGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gestion des stockages\n",
        "- Définition des constantes\n",
        "- Copie dans l'environnement des fichiers en cache chiffré sur Google Drive"
      ],
      "metadata": {
        "id": "yXK2j5wISLfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Monter le drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Définir les stockages\n",
        "data_persistent_dir_base = '/content/drive/MyDrive/image-heberg.fr/'\n",
        "data_persistent_key = data_persistent_dir_base + 'key.fernet'\n",
        "data_persistent_key_web = data_persistent_dir_base + 'key.web'\n",
        "data_crypted_persistent_dir_images = data_persistent_dir_base + 'images/'\n",
        "data_persistent_dir_models = data_persistent_dir_base + 'models/'\n",
        "data_versatile_dir_images = '/data/images/'\n",
        "\n",
        "# Catégories d'images\n",
        "cat_image_approuvee = 'approuvee'\n",
        "cat_image_bloquee = 'bloquee'\n",
        "\n",
        "\n",
        "# Création d'un dossier si nécessaire\n",
        "def create_folder_if_required(path, name):\n",
        "    folder = path + name\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "        print('Création du dossier', folder)\n",
        "\n",
        "\n",
        "# Création des dossiers si nécessaire\n",
        "create_folder_if_required(data_versatile_dir_images, cat_image_approuvee)\n",
        "create_folder_if_required(data_versatile_dir_images, cat_image_bloquee)\n",
        "create_folder_if_required(data_crypted_persistent_dir_images, cat_image_approuvee)\n",
        "create_folder_if_required(data_crypted_persistent_dir_images, cat_image_bloquee)\n",
        "\n",
        "# Création de la clef de chiffrement si nécessaire\n",
        "if not os.path.isfile(data_persistent_key):\n",
        "    key = Fernet.generate_key()\n",
        "    print('Génération de la clef de chiffrment pour Google Drive', key)\n",
        "    file = open(data_persistent_key, 'wb')\n",
        "    file.write(key)\n",
        "    file.close()\n",
        "\n",
        "# Chargement de la clef de chiffrement\n",
        "file = open(data_persistent_key, 'rb')\n",
        "encryption_key = file.read()\n",
        "file.close()\n",
        "\n",
        "\n",
        "# Déchiffrer et copier des fichiers\n",
        "def uncrypt_and_copy(src_file, dst_file, encryption_key):\n",
        "    if not os.path.isfile(src_file):\n",
        "        print('Fichier inexistant :', src_file)\n",
        "        return\n",
        "    # Si le fichier dst existe déjà, ne pas redéchiffrer\n",
        "    if os.path.isfile(dst_file):\n",
        "        return\n",
        "\n",
        "    # Lire le fichier source\n",
        "    file = open(src_file, 'rb')\n",
        "    src_content = file.read()\n",
        "    file.close()\n",
        "\n",
        "    # Chargement de la clef de chiffrement\n",
        "    f = Fernet(encryption_key)\n",
        "\n",
        "    # Ecrire dans le fichier de destination\n",
        "    file = open(dst_file, 'wb')\n",
        "    file.write(f.decrypt(src_content))\n",
        "    file.close()\n",
        "    print(src_file, '=>', dst_file)\n",
        "\n",
        "\n",
        "# Copier en local les données chiffrées du Drive\n",
        "# Images approuvées\n",
        "for image in os.scandir(data_crypted_persistent_dir_images + cat_image_approuvee):\n",
        "    uncrypt_and_copy(data_crypted_persistent_dir_images + cat_image_approuvee + '/' + image.name,\n",
        "                     data_versatile_dir_images + cat_image_approuvee + '/' + image.name, encryption_key)\n",
        "\n",
        "# Images bloquées\n",
        "for image in os.scandir(data_crypted_persistent_dir_images + cat_image_bloquee):\n",
        "    uncrypt_and_copy(data_crypted_persistent_dir_images + cat_image_bloquee + '/' + image.name,\n",
        "                     data_versatile_dir_images + cat_image_bloquee + '/' + image.name, encryption_key)\n",
        "\n",
        "# Nombres de fichiers\n",
        "print(cat_image_approuvee, ':', len([entry for entry in os.listdir(data_versatile_dir_images + cat_image_approuvee)]))\n",
        "print(cat_image_bloquee, ':', len([entry for entry in os.listdir(data_versatile_dir_images + cat_image_bloquee)]))"
      ],
      "metadata": {
        "id": "1gEOiW3KSLvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Récupération des images bloquées / approuvées\n",
        "- Téléchargement de la liste des images depuis image-heberg.fr\n",
        "- Récupération des images non en cache\n",
        "- Mise en cache des images sur Google Drive avec un stockage chiffré (respect des CGU)\n"
      ],
      "metadata": {
        "id": "rxEyMZJvHYdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Télécharger des images à partir d'une URL de liste d'images avec vérification du md5 du fichier\n",
        "def downloadListeImageFromURL(URL_liste_images, categorie_image, encryption_key):\n",
        "    nb_images_total = 0\n",
        "    nb_images_dl = 0\n",
        "    liste_images = requests.get(URL_liste_images, stream=True).raw\n",
        "    for une_image in liste_images:\n",
        "        [img_md5, img_url] = une_image.strip().decode('utf-8').split('###', 1)\n",
        "        img_url_parsee = urlparse(img_url)\n",
        "        img_name = os.path.basename(img_url_parsee.path)\n",
        "        img_dest_versatile = data_versatile_dir_images + categorie_image + '/' + img_name\n",
        "\n",
        "        # Si l'image est déjà présente (en cache via le Drive), ne pas la re-télécharger\n",
        "        if not os.path.isfile(img_dest_versatile):\n",
        "            img_dest_persistent = data_crypted_persistent_dir_images + categorie_image + '/' + img_name\n",
        "            print('Téléchargement de', img_name, '=>', img_dest_versatile)\n",
        "\n",
        "            try:\n",
        "                urlretrieve(img_url, img_dest_versatile)\n",
        "            except (IncompleteRead, ConnectionResetError, URLError) as e:\n",
        "                # OVH limite le nombre de requêtes sur une volumétrie / durée\n",
        "                # => En cas de fermeture de la connexion, faire une pause\n",
        "                print('Exception sur les données reçues', e)\n",
        "                time.sleep(45)\n",
        "                continue  # continue to next row\n",
        "\n",
        "            # L'ajouter au cache chiffré (Google Drive)\n",
        "            # Lire le fichier source\n",
        "            file = open(img_dest_versatile, 'rb')\n",
        "            src_content = file.read()\n",
        "            file.close()\n",
        "\n",
        "            # Chargement de la clef de chiffrement\n",
        "            f = Fernet(encryption_key)\n",
        "\n",
        "            # Ecrire dans le fichier de destination\n",
        "            file = open(img_dest_persistent, 'wb')\n",
        "            file.write(f.encrypt(src_content))\n",
        "            file.close()\n",
        "\n",
        "            # Incrémenter le compteur\n",
        "            nb_images_dl += 1\n",
        "\n",
        "        ## DEBUG - Afficher l'image ##\n",
        "        #import matplotlib.pyplot as plt\n",
        "        #import matplotlib.image as mpimg\n",
        "        #img = mpimg.imread(img_dest_versatile) #Replace \"image.jpg\" with the path of your image\n",
        "        #plt.imshow(img)\n",
        "        #plt.axis('off')\n",
        "        #plt.show()\n",
        "\n",
        "\n",
        "        # Vérification du hash de l'image\n",
        "        file = open(img_dest_versatile, 'rb')\n",
        "        file_md5 = hashlib.file_digest(file, 'md5')\n",
        "        file.close()\n",
        "        if not img_md5 == file_md5.hexdigest():\n",
        "            print('Erreur de hash pour', img_name, ': calculé', file_md5.hexdigest(), '- attendu :', img_md5)\n",
        "            os.remove(file)\n",
        "\n",
        "        # Incrémenter le compteur\n",
        "        nb_images_total += 1\n",
        "\n",
        "    print(categorie_image, ': ', nb_images_total, '(dont', nb_images_dl, 'téléchargées)')\n",
        "\n",
        "\n",
        "# Récupération de la clef pour communiquer avec image-heberg.fr\n",
        "# Lire le fichier source\n",
        "file = open(data_persistent_key_web, 'r')\n",
        "key_web = file.read()\n",
        "file.close()\n",
        "\n",
        "# Images bloquées\n",
        "downloadListeImageFromURL('https://www.image-heberg.fr/ia.php?etat=bloque&key=' + key_web, cat_image_bloquee,\n",
        "                          encryption_key)\n",
        "# Images approuvées\n",
        "downloadListeImageFromURL('https://www.image-heberg.fr/ia.php?etat=approuvee&key=' + key_web, cat_image_approuvee,\n",
        "                          encryption_key)\n"
      ],
      "metadata": {
        "id": "xQyL0xuwMSwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vérification de la qualité des données"
      ],
      "metadata": {
        "id": "Y-Mj7dv5JJM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yourDirectory = data_crypted_persistent_dir_images + cat_image_approuvee + '/'\n",
        "for filename in os.scandir(yourDirectory):\n",
        "  if filename.name.endswith(\".jpg\"):\n",
        "    print(yourDirectory+filename.name)\n",
        "    cv2.imread(yourDirectory+filename.name)\n",
        "yourDirectory = data_crypted_persistent_dir_images + cat_image_bloquee + '/'\n",
        "for filename in os.scandir(yourDirectory):\n",
        "  if filename.name.endswith(\".jpg\"):\n",
        "    print(yourDirectory+filename.name)\n",
        "    cv2.imread(yourDirectory+filename.name)"
      ],
      "metadata": {
        "id": "W1luGPFEKbex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from struct import unpack\n",
        "\n",
        "marker_mapping = {\n",
        "    0xffd8: \"Start of Image\",\n",
        "    0xffe0: \"Application Default Header\",\n",
        "    0xffdb: \"Quantization Table\",\n",
        "    0xffc0: \"Start of Frame\",\n",
        "    0xffc4: \"Define Huffman Table\",\n",
        "    0xffda: \"Start of Scan\",\n",
        "    0xffd9: \"End of Image\"\n",
        "}\n",
        "\n",
        "\n",
        "class JPEG:\n",
        "    def __init__(self, image_file):\n",
        "        with open(image_file, 'rb') as f:\n",
        "            self.img_data = f.read()\n",
        "\n",
        "    def decode(self):\n",
        "        data = self.img_data\n",
        "        while(True):\n",
        "            marker, = unpack(\">H\", data[0:2])\n",
        "            # print(marker_mapping.get(marker))\n",
        "            if marker == 0xffd8:\n",
        "                data = data[2:]\n",
        "            elif marker == 0xffd9:\n",
        "                return\n",
        "            elif marker == 0xffda:\n",
        "                data = data[-2:]\n",
        "            else:\n",
        "                lenchunk, = unpack(\">H\", data[2:4])\n",
        "                data = data[2+lenchunk:]\n",
        "            if len(data)==0:\n",
        "                break\n",
        "\n",
        "\n",
        "bads = []\n",
        "\n",
        "# Images approuvées\n",
        "for image in os.scandir(data_crypted_persistent_dir_images + cat_image_approuvee):\n",
        "    try:\n",
        "        mon_image = JPEG()\n",
        "        image.decode()\n",
        "    except:\n",
        "        bads.append(img)\n",
        "    uncrypt_and_copy(data_crypted_persistent_dir_images + cat_image_approuvee + '/' + image.name,\n",
        "                     data_versatile_dir_images + cat_image_approuvee + '/' + image.name, encryption_key)\n",
        "\n",
        "# Images bloquées\n",
        "for image in os.scandir(data_crypted_persistent_dir_images + cat_image_bloquee):\n",
        "    uncrypt_and_copy(data_crypted_persistent_dir_images + cat_image_bloquee + '/' + image.name,\n",
        "                     data_versatile_dir_images + cat_image_bloquee + '/' + image.name, encryption_key)\n",
        "\n",
        "for img in tqdm(images):\n",
        "  image = osp.join(root_img,img)\n",
        "  image = JPEG(image)\n",
        "\n",
        "\n",
        "\n",
        "for name in bads:\n",
        "  print(name)\n",
        "  #os.remove(osp.join(root_img,name))"
      ],
      "metadata": {
        "id": "0_dIFRUaJJpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Génération des jeux d'entrainement et de validation"
      ],
      "metadata": {
        "id": "ZZeZY4phTxPA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI-zwYp-koHL"
      },
      "source": [
        "# Traitements des images par lots pour accélérer le traitement\n",
        "batch_size = 5\n",
        "# Taille des images sur lequel le traitement sera effectué (plus elle est grande, plus le temps de calcul est long)\n",
        "img_height = 200\n",
        "img_width = 200\n",
        "# Taille de l'échantillon de validation (0 - 1)\n",
        "validation = 0.2\n",
        "# Avoir des traces d'erreur verbeuses\n",
        "#tf.debugging.disable_traceback_filtering()\n",
        "\n",
        "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_versatile_dir_images,\n",
        "    validation_split=validation,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_versatile_dir_images,\n",
        "    validation_split=validation,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "# Valeurs possibles en sortie\n",
        "class_names = val_data.class_names\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Affichage d'un sample des images"
      ],
      "metadata": {
        "id": "fagvKtM6Tuup"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2aSfXtToyru"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_data.take(1):\n",
        "    for i in range(batch_size):\n",
        "        ax = plt.subplot(1, batch_size, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition et entraînement du CNN"
      ],
      "metadata": {
        "id": "7ugHsfm9UCl4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iETojt-6qACe"
      },
      "source": [
        "# Nombre de classes possibles (Approuvée, Bloquée)\n",
        "num_classes = 2\n",
        "# Nombre de tours d'entrainement du modele\n",
        "nb_epochs = 10\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Rescaling(1. / 255),\n",
        "    layers.Conv2D(128, 4, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 4, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(32, 4, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(16, 4, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'], )\n",
        "\n",
        "logdir = \"logs\"\n",
        "\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1, write_images=logdir)\n",
        "\n",
        "model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=nb_epochs,\n",
        "    callbacks=[tensorboard_callback]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frjQUxl3GL3d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interface de test du CNN généré"
      ],
      "metadata": {
        "id": "3Da_IT1HSpMy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTEMBqJoaVSZ"
      },
      "source": [
        "file_to_predict = files.upload()\n",
        "for file_ in file_to_predict:\n",
        "    image_to_predict = cv2.imread(file_, cv2.IMREAD_COLOR)\n",
        "    plt.imshow(cv2.cvtColor(image_to_predict, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "    img_to_predict = np.expand_dims(cv2.resize(image_to_predict, (200, 200)), axis=0)\n",
        "    predict_x = model.predict(img_to_predict)\n",
        "    res = np.argmax(predict_x, axis=1)\n",
        "    # [proba Approuvée, proba Bloquée]\n",
        "    print(predict_x)\n",
        "    print(res)\n",
        "\n",
        "    if res == 1:\n",
        "        print(\"BLOQUEE\")\n",
        "    elif res == 0:\n",
        "        print(\"APPROUVEE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export du modèle vers tensorflowjs"
      ],
      "metadata": {
        "id": "9mUTjghEfBcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfjs.converters.save_keras_model(model, data_persistent_dir_models)"
      ],
      "metadata": {
        "id": "Ew-lCjbdfD4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dashboard des logs du CNN"
      ],
      "metadata": {
        "id": "CXQr8u27b7bw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OnHSaQxrigA"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}